machine
learning
ml
field
study
artificial
intelligence
concerned
development
study
statistical
algorithms
learn
from
data
generalise
unseen
data
thus
perform
tasks
without
explicit
instructions


within
subdiscipline
machine
learning
advances
field
deep
learning
allowed
neural
networks
class
statistical
algorithms
surpass
many
previous
machine
learning
approaches
performance


ml
finds
application
many
fields
including
natural
language
processing
computer
vision
speech
recognition
email
filtering
agriculture
medicine
application
ml
business
problems
known
as
predictive
analytics
statistics
mathematical
optimisation
mathematical
programming
methods
comprise
foundations
machine
learning
data
mining
related
field
study
focusing
exploratory
data
analysis
eda
via
unsupervised
learning




from
theoretical
viewpoint
probably
approximately
correct
learning
provides
framework
describing
machine
learning
term
machine
learning
coined


arthur
samuel
ibm
employee
pioneer
field
computer
gaming
artificial
intelligence




synonym
self
teaching
computers
also
used
time
period




earliest
machine
learning
program
introduced

s
when
arthur
samuel
invented
computer
program
calculated
winning
chance
checkers
each
side
history
machine
learning
roots
back
decades
human
desire
effort
study
human
cognitive
processes




canadian
psychologist
donald
hebb
published
book
organization
behavior
which
introduced
theoretical
neural
structure
formed
certain
interactions
among
nerve
cells


hebb
s
model
neurons
interacting
one
another
set
groundwork
how
ais
machine
learning
algorithms
work
under
nodes
artificial
neurons
used
computers
communicate
data


other
researchers
who
studied
human
cognitive
systems
contributed
modern
machine
learning
technologies
as
well
including
logician
walter
pitts
warren
mcculloch
who
proposed
early
mathematical
models
neural
networks
come
up
algorithms
mirror
human
thought
processes


early

s
experimental
learning
machine
punched
tape
memory
called
cybertron
developed
raytheon
company
analyse
sonar
signals
electrocardiograms
speech
patterns
using
rudimentary
reinforcement
learning
repetitively
trained
human
operator
teacher
recognise
patterns
equipped
goof
button
cause
reevaluate
incorrect
decisions


representative
book
research
into
machine
learning
during

s
nilsson
s
book
learning
machines
dealing
mostly
machine
learning
pattern
classification


interest
related
pattern
recognition
continued
into

s
as
described
duda
hart






report
given
using
teaching
strategies
so
artificial
neural
network
learns
recognise


characters


letters


digits


special
symbols
from
computer
terminal


tom
m
mitchell
provided
widely
quoted
more
formal
definition
algorithms
studied
machine
learning
field
computer
program
said
learn
from
experience
e
respect
some
class
tasks
t
performance
measure
p
if
its
performance
tasks
t
as
measured
p
improves
experience
e


definition
tasks
which
machine
learning
concerned
offers
fundamentally
operational
definition
rather
than
defining
field
cognitive
terms
follows
alan
turing
s
proposal
his
paper
computing
machinery
intelligence
which
question
machines
think
replaced
question
machines
what
as
thinking
entities


modern
day
machine
learning
algorithms
broken
into


algorithms
types
supervised
learning
algorithms
unsupervised
learning
algorithms
reinforcement
learning
algorithms


as
scientific
endeavour
machine
learning
grew
out
quest
artificial
intelligence
ai
early
days
ai
as
academic
discipline
some
researchers
interested
having
machines
learn
from
data
attempted
approach
problem
various
symbolic
methods
as
well
as
what
then
termed
neural
networks
these
mostly
perceptrons
other
models
later
found
reinventions
generalised
linear
models
statistics


probabilistic
reasoning
also
employed
especially
automated
medical
diagnosis




however
increasing
emphasis
logical
knowledge
based
approach
caused
rift
between
ai
machine
learning
probabilistic
systems
plagued
theoretical
practical
problems
data
acquisition
representation






expert
systems
come
dominate
ai
statistics
out
favour


work
symbolic
knowledge
based
learning
continue
within
ai
leading
inductive
logic
programming
ilp
more
statistical
line
research
now
outside
field
ai
proper
pattern
recognition
information
retrieval








neural
networks
research
abandoned
ai
computer
science
around
same
time
line
too
continued
outside
ai
cs
field
as
connectionism
researchers
from
other
disciplines
including
john
hopfield
david
rumelhart
geoffrey
hinton
their
main
success
came
mid

s
reinvention
backpropagation




machine
learning
ml
reorganised
recognised
as
its
own
field
started
flourish

s
field
changed
its
goal
from
achieving
artificial
intelligence
tackling
solvable
problems
practical
nature
shifted
focus
away
from
symbolic
approaches
inherited
from
ai
toward
methods
models
borrowed
from
statistics
fuzzy
logic
probability
theory


there
close
connection
between
machine
learning
compression
system
predicts
posterior
probabilities
sequence
given
its
entire
history
used
optimal
data
compression
using
arithmetic
coding
output
distribution
conversely
optimal
compressor
used
prediction
finding
symbol
compresses
best
given
previous
history
equivalence
used
as
justification
using
data
compression
as
benchmark
general
intelligence






alternative
view
show
compression
algorithms
implicitly
map
strings
into
implicit
feature
space
vectors
compression
based
similarity
measures
compute
similarity
within
these
feature
spaces
each
compressor
c
define
associated
vector
space
such
c
maps
input
string
x
corresponding
vector
norm
x
exhaustive
examination
feature
spaces
underlying
all
compression
algorithms
precluded
space
instead
feature
vectors
chooses
examine
three
representative
lossless
compression
methods
lzw
lz

ppm


according
aixi
theory
connection
more
directly
explained
hutter
prize
best
possible
compression
x
smallest
possible
software
generates
x
example
model
zip
file
s
compressed
size
includes
both
zip
file
unzipping
software
since
not
unzip
without
both
there
even
smaller
combined
form
examples
ai
powered
audio
video
compression
software
include
nvidia
maxine
aivc


examples
software
perform
ai
powered
image
compression
include
opencv
tensorflow
matlab
s
image
processing
toolbox
ipt
high
fidelity
generative
image
compression


unsupervised
machine
learning
k
means
clustering
utilized
compress
data
grouping
similar
data
points
into
clusters
technique
simplifies
handling
extensive
datasets
lack
predefined
labels
finds
widespread
use
fields
such
as
image
compression


data
compression
aims
reduce
size
data
files
enhancing
storage
efficiency
speeding
up
data
transmission
k
means
clustering
unsupervised
machine
learning
algorithm
employed
partition
dataset
into
specified
number
clusters
k
each
represented
centroid
its
points
process
condenses
extensive
datasets
into
more
compact
set
representative
points
particularly
beneficial
image
signal
processing
k
means
clustering
aids
data
reduction
replacing
groups
data
points
their
centroids
thereby
preserving
core
information
original
data
while
significantly
decreasing
required
storage
space


machine
learning
data
mining
often
employ
same
methods
overlap
significantly
while
machine
learning
focuses
prediction
based
known
properties
learned
from
training
data
data
mining
focuses
discovery
previously
unknown
properties
data
analysis
step
knowledge
discovery
databases
data
mining
uses
many
machine
learning
methods
different
goals
other
hand
machine
learning
also
employs
data
mining
methods
as
unsupervised
learning
as
preprocessing
step
improve
learner
accuracy
much
confusion
between
these
two
research
communities
which
often
separate
conferences
separate
journals
ecml
pkdd
major
exception
comes
from
basic
assumptions
work
machine
learning
performance
usually
evaluated
respect
ability
reproduce
known
knowledge
while
knowledge
discovery
data
mining
kdd
key
task
discovery
previously
unknown
knowledge
evaluated
respect
known
knowledge
uninformed
unsupervised
method
easily
outperformed
other
supervised
methods
while
typical
kdd
task
supervised
methods
cannot
used
due
unavailability
training
data
machine
learning
also
intimate
ties
optimisation
many
learning
problems
formulated
as
minimisation
some
loss
function
training
set
examples
loss
functions
express
discrepancy
between
predictions
model
trained
actual
problem
instances
example
classification
one
wants
assign
label
instances
models
trained
correctly
predict
preassigned
labels
set
examples


characterizing
generalisation
various
learning
algorithms
active
topic
current
research
especially
deep
learning
algorithms
machine
learning
statistics
closely
related
fields
terms
methods
distinct
their
principal
goal
statistics
draws
population
inferences
from
sample
while
machine
learning
finds
generalisable
predictive
patterns


conventional
statistical
analyses
require
priori
selection
model
most
suitable
study
data
set
addition
only
significant
theoretically
relevant
variables
based
previous
experience
included
analysis
contrast
machine
learning
not
built
pre
structured
model
rather
data
shape
model
detecting
underlying
patterns
more
variables
input
used
train
model
more
accurate
ultimate
model


leo
breiman
distinguished
two
statistical
modelling
paradigms
data
model
algorithmic
model


wherein
algorithmic
model
means
more
less
machine
learning
algorithms
like
random
forest
some
statisticians
adopted
methods
from
machine
learning
leading
combined
field
call
statistical
learning


analytical
computational
techniques
derived
from
deep
rooted
physics
disordered
systems
extended
large
scale
problems
including
machine
learning
e
g
analyse
weight
space
deep
neural
networks


statistical
physics
thus
finding
applications
area
medical
diagnostics


core
objective
learner
generalise
from
its
experience




generalisation
context
ability
learning
machine
perform
accurately
new
unseen
examples
tasks
after
having
experienced
learning
data
set
training
examples
come
from
some
generally
unknown
probability
distribution
considered
representative
space
occurrences
learner
build
general
model
space
enables
produce
sufficiently
accurate
predictions
new
cases
computational
analysis
machine
learning
algorithms
their
performance
branch
theoretical
computer
science
known
as
computational
learning
theory
via
probably
approximately
correct
learning
model
because
training
sets
finite
future
uncertain
learning
theory
usually
not
yield
guarantees
performance
algorithms
instead
probabilistic
bounds
performance
quite
common
bias
variance
decomposition
one
way
quantify
generalisation
error
best
performance
context
generalisation
complexity
hypothesis
match
complexity
function
underlying
data
if
hypothesis
less
complex
than
function
then
model
under
fitted
data
if
complexity
model
increased
response
then
training
error
decreases
if
hypothesis
too
complex
then
model
subject
overfitting
generalisation
poorer


addition
performance
bounds
learning
theorists
study
time
complexity
feasibility
learning
computational
learning
theory
computation
considered
feasible
if
done
polynomial
time
there
two
kinds
time
complexity
results
positive
results
show
certain
class
functions
learned
polynomial
time
negative
results
show
certain
classes
cannot
learned
polynomial
time
machine
learning
approaches
traditionally
divided
into
three
broad
categories
which
correspond
learning
paradigms
depending
nature
signal
feedback
available
learning
system
although
each
algorithm
advantages
limitations
no
single
algorithm
works
all
problems






supervised
learning
algorithms
build
mathematical
model
set
data
contains
both
inputs
desired
outputs


data
known
as
training
data
consists
set
training
examples
each
training
example
one
more
inputs
desired
output
also
known
as
supervisory
signal
mathematical
model
each
training
example
represented
array
vector
sometimes
called
feature
vector
training
data
represented
matrix
through
iterative
optimisation
objective
function
supervised
learning
algorithms
learn
function
used
predict
output
associated
new
inputs


optimal
function
allows
algorithm
correctly
determine
output
inputs
not
part
training
data
algorithm
improves
accuracy
its
outputs
predictions
over
time
said
learned
perform
task


types
supervised
learning
algorithms
include
active
learning
classification
regression


classification
algorithms
used
when
outputs
restricted
limited
set
values
while
regression
algorithms
used
when
outputs
take
any
numerical
value
within
range
example
classification
algorithm
filters
emails
input
incoming
email
output
folder
which
file
email
contrast
regression
used
tasks
such
as
predicting
person
s
height
based
factors
like
age
genetics
forecasting
future
temperatures
based
historical
data


similarity
learning
area
supervised
machine
learning
closely
related
regression
classification
goal
learn
from
examples
using
similarity
function
measures
how
similar
related
two
objects
applications
ranking
recommendation
systems
visual
identity
tracking
face
verification
speaker
verification
unsupervised
learning
algorithms
find
structures
data
not
labelled
classified
categorised
instead
responding
feedback
unsupervised
learning
algorithms
identify
commonalities
data
react
based
presence
absence
such
commonalities
each
new
piece
data
central
applications
unsupervised
machine
learning
include
clustering
dimensionality
reduction


density
estimation


cluster
analysis
assignment
set
observations
into
subsets
called
clusters
so
observations
within
same
cluster
similar
according
one
more
predesignated
criteria
while
observations
drawn
from
different
clusters
dissimilar
different
clustering
techniques
make
different
assumptions
structure
data
often
defined
some
similarity
metric
evaluated
example
internal
compactness
similarity
between
members
same
cluster
separation
difference
between
clusters
other
methods
based
estimated
density
graph
connectivity
special
type
unsupervised
learning
called
self
supervised
learning
involves
training
model
generating
supervisory
signal
from
data
itself




semi
supervised
learning
falls
between
unsupervised
learning
without
any
labelled
training
data
supervised
learning
completely
labelled
training
data
some
training
examples
missing
training
labels
yet
many
machine
learning
researchers
found
unlabelled
data
when
used
conjunction
small
amount
labelled
data
produce
considerable
improvement
learning
accuracy
weakly
supervised
learning
training
labels
noisy
limited
imprecise
however
these
labels
often
cheaper
obtain
resulting
larger
effective
training
sets


reinforcement
learning
area
machine
learning
concerned
how
software
agents
ought
take
actions
environment
so
as
maximise
some
notion
cumulative
reward
due
its
generality
field
studied
many
other
disciplines
such
as
game
theory
control
theory
operations
research
information
theory
simulation
based
optimisation
multi
agent
systems
swarm
intelligence
statistics
genetic
algorithms
reinforcement
learning
environment
typically
represented
as
markov
decision
process
mdp
many
reinforcement
learning
algorithms
use
dynamic
programming
techniques


reinforcement
learning
algorithms
not
assume
knowledge
exact
mathematical
model
mdp
used
when
exact
models
infeasible
reinforcement
learning
algorithms
used
autonomous
vehicles
learning
play
game
against
human
opponent
dimensionality
reduction
process
reducing
number
random
variables
under
consideration
obtaining
set
principal
variables


other
words
process
reducing
dimension
feature
set
also
called
number
features
most
dimensionality
reduction
techniques
considered
as
either
feature
elimination
extraction
one
popular
methods
dimensionality
reduction
principal
component
analysis
pca
pca
involves
changing
higher
dimensional
data
e
g

d
smaller
space
e
g

d
manifold
hypothesis
proposes
high
dimensional
data
sets
lie
along
low
dimensional
manifolds
many
dimensionality
reduction
techniques
make
assumption
leading
area
manifold
learning
manifold
regularisation
other
approaches
developed
which
not
fit
neatly
into
three
fold
categorisation
sometimes
more
than
one
used
same
machine
learning
system
example
topic
modelling
meta
learning


self
learning
as
machine
learning
paradigm
introduced


along
neural
network
capable
self
learning
named
crossbar
adaptive
array
caa




gives
solution
problem
learning
without
any
external
reward
introducing
emotion
as
internal
reward
emotion
used
as
state
evaluation
self
learning
agent
caa
self
learning
algorithm
computes
crossbar
fashion
both
decisions
actions
emotions
feelings
consequence
situations
system
driven
interaction
between
cognition
emotion


self
learning
algorithm
updates
memory
matrix
w
w
s
such
each
iteration
executes
following
machine
learning
routine
system
only
one
input
situation
only
one
output
action
behaviour
there
neither
separate
reinforcement
input
nor
advice
input
from
environment
backpropagated
value
secondary
reinforcement
emotion
toward
consequence
situation
caa
exists
two
environments
one
behavioural
environment
where
behaves
other
genetic
environment
wherefrom
initially
only
once
receives
initial
emotions
situations
encountered
behavioural
environment
after
receiving
genome
species
vector
from
genetic
environment
caa
learns
goal
seeking
behaviour
environment
contains
both
desirable
undesirable
situations


several
learning
algorithms
aim
discovering
better
representations
inputs
provided
during
training


classic
examples
include
principal
component
analysis
cluster
analysis
feature
learning
algorithms
also
called
representation
learning
algorithms
often
attempt
preserve
information
their
input
also
transform
way
makes
useful
often
as
pre
processing
step
before
performing
classification
predictions
technique
allows
reconstruction
inputs
coming
from
unknown
data
generating
distribution
while
not
necessarily
faithful
configurations
implausible
under
distribution
replaces
manual
feature
engineering
allows
machine
both
learn
features
use
perform
specific
task
feature
learning
either
supervised
unsupervised
supervised
feature
learning
features
learned
using
labelled
input
data
examples
include
artificial
neural
networks
multilayer
perceptrons
supervised
dictionary
learning
unsupervised
feature
learning
features
learned
unlabelled
input
data
examples
include
dictionary
learning
independent
component
analysis
autoencoders
matrix
factorisation


various
forms
clustering






manifold
learning
algorithms
attempt
so
under
constraint
learned
representation
low
dimensional
sparse
coding
algorithms
attempt
so
under
constraint
learned
representation
sparse
meaning
mathematical
model
many
zeros
multilinear
subspace
learning
algorithms
aim
learn
low
dimensional
representations
directly
from
tensor
representations
multidimensional
data
without
reshaping
into
higher
dimensional
vectors


deep
learning
algorithms
discover
multiple
levels
representation
hierarchy
features
higher
level
more
abstract
features
defined
terms
generating
lower
level
features
argued
intelligent
machine
one
learns
representation
disentangles
underlying
factors
variation
explain
observed
data


feature
learning
motivated
fact
machine
learning
tasks
such
as
classification
often
require
input
mathematically
computationally
convenient
process
however
real
world
data
such
as
images
video
sensory
data
not
yielded
attempts
algorithmically
define
specific
features
alternative
discover
such
features
representations
through
examination
without
relying
explicit
algorithms
sparse
dictionary
learning
feature
learning
method
where
training
example
represented
as
linear
combination
basis
functions
assumed
sparse
matrix
method
strongly
np
hard
difficult
solve
approximately


popular
heuristic
method
sparse
dictionary
learning
k
svd
algorithm
sparse
dictionary
learning
applied
several
contexts
classification
problem
determine
class
which
previously
unseen
training
example
belongs
dictionary
where
each
class
already
built
new
training
example
associated
class
best
sparsely
represented
corresponding
dictionary
sparse
dictionary
learning
also
applied
image
de
noising
key
idea
clean
image
patch
sparsely
represented
image
dictionary
noise
cannot


data
mining
anomaly
detection
also
known
as
outlier
detection
identification
rare
items
events
observations
which
raise
suspicions
differing
significantly
from
majority
data


typically
anomalous
items
represent
issue
such
as
bank
fraud
structural
defect
medical
problems
errors
text
anomalies
referred
as
outliers
novelties
noise
deviations
exceptions


particular
context
abuse
network
intrusion
detection
interesting
objects
often
not
rare
objects
unexpected
bursts
inactivity
pattern
not
adhere
common
statistical
definition
outlier
as
rare
object
many
outlier
detection
methods
particular
unsupervised
algorithms
fail
such
data
unless
aggregated
appropriately
instead
cluster
analysis
algorithm
able
detect
micro
clusters
formed
these
patterns


three
broad
categories
anomaly
detection
techniques
exist


unsupervised
anomaly
detection
techniques
detect
anomalies
unlabelled
test
data
set
under
assumption
majority
instances
data
set
normal
looking
instances
seem
fit
least
remainder
data
set
supervised
anomaly
detection
techniques
require
data
set
labelled
as
normal
abnormal
involves
training
classifier
key
difference
from
many
other
statistical
classification
problems
inherently
unbalanced
nature
outlier
detection
semi
supervised
anomaly
detection
techniques
construct
model
representing
normal
behaviour
from
given
normal
training
data
set
then
test
likelihood
test
instance
generated
model
robot
learning
inspired
multitude
machine
learning
methods
starting
from
supervised
learning
reinforcement
learning




finally
meta
learning
e
g
maml
association
rule
learning
rule
based
machine
learning
method
discovering
relationships
between
variables
large
databases
intended
identify
strong
rules
discovered
databases
using
some
measure
interestingness


rule
based
machine
learning
general
term
any
machine
learning
method
identifies
learns
evolves
rules
store
manipulate
apply
knowledge
defining
characteristic
rule
based
machine
learning
algorithm
identification
utilisation
set
relational
rules
collectively
represent
knowledge
captured
system
contrast
other
machine
learning
algorithms
commonly
identify
singular
model
universally
applied
any
instance
order
make
prediction


rule
based
machine
learning
approaches
include
learning
classifier
systems
association
rule
learning
artificial
immune
systems
based
concept
strong
rules
rakesh
agrawal
tomasz
imieli
ski
arun
swami
introduced
association
rules
discovering
regularities
between
products
large
scale
transaction
data
recorded
point
sale
pos
systems
supermarkets


example
rule
o
n
o
n
s
p
o
t
t
o
e
s
b
u
r
g
e
r
displaystyle
mathrm
onions
potatoes
rightarrow
mathrm
burger
found
sales
data
supermarket
indicate
if
customer
buys
onions
potatoes
together
likely
also
buy
hamburger
meat
such
information
used
as
basis
decisions
marketing
activities
such
as
promotional
pricing
product
placements
addition
market
basket
analysis
association
rules
employed
today
application
areas
including
web
usage
mining
intrusion
detection
continuous
production
bioinformatics
contrast
sequence
mining
association
rule
learning
typically
not
consider
order
items
either
within
transaction
across
transactions
learning
classifier
systems
lcs
family
rule
based
machine
learning
algorithms
combine
discovery
component
typically
genetic
algorithm
learning
component
performing
either
supervised
learning
reinforcement
learning
unsupervised
learning
seek
identify
set
context
dependent
rules
collectively
store
apply
knowledge
piecewise
manner
order
make
predictions


inductive
logic
programming
ilp
approach
rule
learning
using
logic
programming
as
uniform
representation
input
examples
background
knowledge
hypotheses
given
encoding
known
background
knowledge
set
examples
represented
as
logical
database
facts
ilp
system
derive
hypothesized
logic
program
entails
all
positive
no
negative
examples
inductive
programming
related
field
considers
any
kind
programming
language
representing
hypotheses
not
only
logic
programming
such
as
functional
programs
inductive
logic
programming
particularly
useful
bioinformatics
natural
language
processing
gordon
plotkin
ehud
shapiro
laid
initial
theoretical
foundation
inductive
machine
learning
logical
setting






shapiro
built
their
first
implementation
model
inference
system


prolog
program
inductively
inferred
logic
programs
from
positive
negative
examples


term
inductive
here
refers
philosophical
induction
suggesting
theory
explain
observed
facts
rather
than
mathematical
induction
proving
property
all
members
well
ordered
set
machine
learning
model
type
mathematical
model
once
trained
given
dataset
used
make
predictions
classifications
new
data
during
training
learning
algorithm
iteratively
adjusts
model
s
internal
parameters
minimise
errors
its
predictions


extension
term
model
refer
several
levels
specificity
from
general
class
models
their
associated
learning
algorithms
fully
trained
model
all
its
internal
parameters
tuned


various
types
models
used
researched
machine
learning
systems
picking
best
model
task
called
model
selection
artificial
neural
networks
anns
connectionist
systems
computing
systems
vaguely
inspired
biological
neural
networks
constitute
animal
brains
such
systems
learn
perform
tasks
considering
examples
generally
without
programmed
any
task
specific
rules
ann
model
based
collection
connected
units
nodes
called
artificial
neurons
which
loosely
model
neurons
biological
brain
each
connection
like
synapses
biological
brain
transmit
information
signal
from
one
artificial
neuron
another
artificial
neuron
receives
signal
process
then
signal
additional
artificial
neurons
connected
common
ann
implementations
signal
connection
between
artificial
neurons
real
number
output
each
artificial
neuron
computed
some
non
linear
function
sum
its
inputs
connections
between
artificial
neurons
called
edges
artificial
neurons
edges
typically
weight
adjusts
as
learning
proceeds
weight
increases
decreases
strength
signal
connection
artificial
neurons
threshold
such
signal
only
sent
if
aggregate
signal
crosses
threshold
typically
artificial
neurons
aggregated
into
layers
different
layers
perform
different
kinds
transformations
their
inputs
signals
travel
from
first
layer
input
layer
last
layer
output
layer
possibly
after
traversing
layers
multiple
times
original
goal
ann
approach
solve
problems
same
way
human
brain
however
over
time
attention
moved
performing
specific
tasks
leading
deviations
from
biology
artificial
neural
networks
used
variety
tasks
including
computer
vision
speech
recognition
machine
translation
social
network
filtering
playing
board
video
games
medical
diagnosis
deep
learning
consists
multiple
hidden
layers
artificial
neural
network
approach
tries
model
way
human
brain
processes
light
sound
into
vision
hearing
some
successful
applications
deep
learning
computer
vision
speech
recognition


decision
tree
learning
uses
decision
tree
as
predictive
model
go
from
observations
item
represented
branches
conclusions
item
s
target
value
represented
leaves
one
predictive
modelling
approaches
used
statistics
data
mining
machine
learning
tree
models
where
target
variable
take
discrete
set
values
called
classification
trees
these
tree
structures
leaves
represent
class
labels
branches
represent
conjunctions
features
lead
those
class
labels
decision
trees
where
target
variable
take
continuous
values
typically
real
numbers
called
regression
trees
decision
analysis
decision
tree
used
visually
explicitly
represent
decisions
decision
making
data
mining
decision
tree
describes
data
resulting
classification
tree
input
decision
making
random
forest
regression
rfr
falls
under
umbrella
decision
tree
based
models
rfr
ensemble
learning
method
builds
multiple
decision
trees
averages
their
predictions
improve
accuracy
avoid
overfitting
build
decision
trees
rfr
uses
bootstrapped
sampling
instance
each
decision
tree
trained
random
data
from
training
set
random
selection
rfr
training
enables
model
reduce
bias
predictions
achieve
higher
degree
accuracy
rfr
generates
independent
decision
trees
work
single
output
data
as
well
multiple
regressor
tasks
makes
rfr
compatible
used
various
applications




support
vector
machines
svms
also
known
as
support
vector
networks
set
related
supervised
learning
methods
used
classification
regression
given
set
training
examples
each
marked
as
belonging
one
two
categories
svm
training
algorithm
builds
model
predicts
whether
new
example
falls
into
one
category


svm
training
algorithm
non
probabilistic
binary
linear
classifier
although
methods
such
as
platt
scaling
exist
use
svm
probabilistic
classification
setting
addition
performing
linear
classification
svms
efficiently
perform
non
linear
classification
using
what
called
kernel
trick
implicitly
mapping
their
inputs
into
high
dimensional
feature
spaces
regression
analysis
encompasses
large
variety
statistical
methods
estimate
relationship
between
input
variables
their
associated
features
its
most
common
form
linear
regression
where
single
line
drawn
best
fit
given
data
according
mathematical
criterion
such
as
ordinary
least
squares
latter
often
extended
regularisation
methods
mitigate
overfitting
bias
as
ridge
regression
when
dealing
non
linear
problems
go
models
include
polynomial
regression
example
used
trendline
fitting
microsoft
excel


logistic
regression
often
used
statistical
classification
even
kernel
regression
which
introduces
non
linearity
taking
advantage
kernel
trick
implicitly
map
input
variables
higher
dimensional
space
multivariate
linear
regression
extends
concept
linear
regression
handle
multiple
dependent
variables
simultaneously
approach
estimates
relationships
between
set
input
variables
several
output
variables
fitting
multidimensional
linear
model
particularly
useful
scenarios
where
outputs
interdependent
share
underlying
patterns
such
as
predicting
multiple
economic
indicators
reconstructing
images


which
inherently
multi
dimensional
bayesian
network
belief
network
directed
acyclic
graphical
model
probabilistic
graphical
model
represents
set
random
variables
their
conditional
independence
directed
acyclic
graph
dag
example
bayesian
network
represent
probabilistic
relationships
between
diseases
symptoms
given
symptoms
network
used
compute
probabilities
presence
various
diseases
efficient
algorithms
exist
perform
inference
learning
bayesian
networks
model
sequences
variables
like
speech
signals
protein
sequences
called
dynamic
bayesian
networks
generalisations
bayesian
networks
represent
solve
decision
problems
under
uncertainty
called
influence
diagrams
gaussian
process
stochastic
process
which
every
finite
collection
random
variables
process
multivariate
normal
distribution
relies
pre
defined
covariance
function
kernel
models
how
pairs
points
relate
each
other
depending
their
locations
given
set
observed
points
input
output
examples
distribution
unobserved
output
new
point
as
function
its
input
data
directly
computed
looking
like
observed
points
covariances
between
those
points
new
unobserved
point
gaussian
processes
popular
surrogate
models
bayesian
optimisation
used
hyperparameter
optimisation
genetic
algorithm
ga
search
algorithm
heuristic
technique
mimics
process
natural
selection
using
methods
such
as
mutation
crossover
generate
new
genotypes
hope
finding
good
solutions
given
problem
machine
learning
genetic
algorithms
used

s

s




conversely
machine
learning
techniques
used
improve
performance
genetic
evolutionary
algorithms


theory
belief
functions
also
referred
as
evidence
theory
dempster
shafer
theory
general
framework
reasoning
uncertainty
understood
connections
other
frameworks
such
as
probability
possibility
imprecise
probability
theories
these
theoretical
frameworks
thought
as
kind
learner
some
analogous
properties
how
evidence
combined
e
g
dempster
s
rule
combination
just
like
how
pmf
based
bayesian
approach
combine
probabilities


however
there
many
caveats
these
beliefs
functions
when
compared
bayesian
approaches
order
incorporate
ignorance
uncertainty
quantification
these
belief
function
approaches
implemented
within
machine
learning
domain
typically
leverage
fusion
approach
various
ensemble
methods
better
handle
learner
s
decision
boundary
low
samples
ambiguous
class
issues
standard
machine
learning
approach
tend
difficulty
resolving




however
computational
complexity
these
algorithms
dependent
number
propositions
classes
lead
much
higher
computation
time
when
compared
other
machine
learning
approaches
rule
based
machine
learning
rbml
branch
machine
learning
automatically
discovers
learns
rules
from
data
provides
interpretable
models
making
useful
decision
making
fields
like
healthcare
fraud
detection
cybersecurity
key
rbml
techniques
includes
learning
classifier
systems


association
rule
learning


artificial
immune
systems


other
similar
models
these
methods
extract
patterns
from
data
evolve
rules
over
time
typically
machine
learning
models
require
high
quantity
reliable
data
perform
accurate
predictions
when
training
machine
learning
model
machine
learning
engineers
need
target
collect
large
representative
sample
data
data
from
training
set
as
varied
as
corpus
text
collection
images
sensor
data
data
collected
from
individual
users
service
overfitting
something
watch
out
when
training
machine
learning
model
trained
models
derived
from
biased
non
evaluated
data
result
skewed
undesired
predictions
biased
models
result
detrimental
outcomes
thereby
furthering
negative
impacts
society
objectives
algorithmic
bias
potential
result
data
not
fully
prepared
training
machine
learning
ethics
becoming
field
study
notably
becoming
integrated
within
machine
learning
engineering
teams
federated
learning
adapted
form
distributed
artificial
intelligence
training
machine
learning
models
decentralises
training
process
allowing
users
privacy
maintained
not
needing
send
their
data
centralised
server
also
increases
efficiency
decentralising
training
process
many
devices
example
gboard
uses
federated
machine
learning
train
search
query
prediction
models
users
mobile
phones
without
having
send
individual
searches
back
google


there
many
applications
machine
learning
including


media
services
provider
netflix
held
first
netflix
prize
competition
find
program
better
predict
user
preferences
improve
accuracy
its
existing
cinematch
movie
recommendation
algorithm
least


joint
team
made
up
researchers
from
t
labs
research
collaboration
teams
big
chaos
pragmatic
theory
built
ensemble
model
win
grand
prize




million


shortly
after
prize
awarded
netflix
realised
viewers
ratings
not
best
indicators
their
viewing
patterns
everything
recommendation
changed
their
recommendation
engine
accordingly




article
wall
street
journal
noted
use
machine
learning
rebellion
research
predict


financial
crisis




co
founder
sun
microsystems
vinod
khosla
predicted


medical
doctors
jobs
lost
next
two
decades
automated
machine
learning
medical
diagnostic
software




reported
machine
learning
algorithm
applied
field
art
history
study
fine
art
paintings
revealed
previously
unrecognised
influences
among
artists




springer
nature
published
first
research
book
created
using
machine
learning




machine
learning
technology
used
help
make
diagnoses
aid
researchers
developing
cure
covid




machine
learning
recently
applied
predict
pro
environmental
behaviour
travellers


recently
machine
learning
technology
also
applied
optimise
smartphone
s
performance
thermal
behaviour
based
user
s
interaction
phone






when
applied
correctly
machine
learning
algorithms
mlas
utilise
wide
range
company
characteristics
predict
stock
returns
without
overfitting
employing
effective
feature
engineering
combining
forecasts
mlas
generate
results
far
surpass
those
obtained
from
basic
linear
techniques
like
ols


recent
advancements
machine
learning
extended
into
field
quantum
chemistry
where
novel
algorithms
now
enable
prediction
solvent
effects
chemical
reactions
thereby
offering
new
tools
chemists
tailor
experimental
conditions
optimal
outcomes


machine
learning
becoming
useful
tool
investigate
predict
evacuation
decision
making
large
scale
small
scale
disasters
different
solutions
tested
predict
if
when
householders
decide
evacuate
during
wildfires
hurricanes






other
applications
focusing
pre
evacuation
decisions
building
fires




although
machine
learning
transformative
some
fields
machine
learning
programs
often
fail
deliver
expected
results






reasons
numerous
lack
suitable
data
lack
access
data
data
bias
privacy
problems
badly
chosen
tasks
algorithms
wrong
tools
people
lack
resources
evaluation
problems


black
box
theory
poses
another
yet
significant
challenge
black
box
refers
situation
where
algorithm
process
producing
output
entirely
opaque
meaning
even
coders
algorithm
cannot
audit
pattern
machine
extracted
out
data


house
lords
select
committee
which
claimed
such
intelligence
system
substantial
impact
individual
s
life
not
considered
acceptable
unless
provided
full
satisfactory
explanation
decisions
makes




self
driving
car
from
uber
failed
detect
pedestrian
who
killed
after
collision


attempts
use
machine
learning
healthcare
ibm
watson
system
failed
deliver
even
after
years
time
billions
dollars
invested




microsoft
s
bing
chat
chatbot
reported
produce
hostile
offensive
response
against
its
users


machine
learning
used
as
strategy
update
evidence
related
systematic
review
increased
reviewer
burden
related
growth
biomedical
literature
while
improved
training
sets
not
yet
developed
sufficiently
reduce
workload
burden
without
limiting
necessary
sensitivity
findings
research
themselves


explainable
ai
xai
interpretable
ai
explainable
machine
learning
xml
artificial
intelligence
ai
which
humans
understand
decisions
predictions
made
ai


contrasts
black
box
concept
machine
learning
where
even
its
designers
cannot
explain
why
ai
arrived
specific
decision


refining
mental
models
users
ai
powered
systems
dismantling
their
misconceptions
xai
promises
help
users
perform
more
effectively
xai
implementation
social
right
explanation
settling
bad
overly
complex
theory
gerrymandered
fit
all
past
training
data
known
as
overfitting
many
systems
attempt
reduce
overfitting
rewarding
theory
accordance
how
well
fits
data
penalising
theory
accordance
how
complex
theory


learners
also
disappoint
learning
wrong
lesson
toy
example
image
classifier
trained
only
pictures
brown
horses
black
cats
conclude
all
brown
patches
likely
horses


real
world
example
unlike
humans
current
image
classifiers
often
not
primarily
make
judgements
from
spatial
relationship
between
components
picture
learn
relationships
between
pixels
humans
oblivious
still
correlate
images
certain
types
real
objects
modifying
these
patterns
legitimate
image
result
adversarial
images
system
misclassifies




adversarial
vulnerabilities
also
result
nonlinear
systems
from
non
pattern
perturbations
some
systems
possible
change
output
only
changing
single
adversarially
chosen
pixel


machine
learning
models
often
vulnerable
manipulation
evasion
via
adversarial
machine
learning


researchers
demonstrated
how
backdoors
placed
undetectably
into
classifying
e
g
categories
spam
well
visible
not
spam
posts
machine
learning
models
often
developed
trained
third
parties
parties
change
classification
any
input
including
cases
which
type
data
software
transparency
provided
possibly
including
white
box
access






classification
machine
learning
models
validated
accuracy
estimation
techniques
like
holdout
method
which
splits
data
training
test
set
conventionally




training
set




test
set
designation
evaluates
performance
training
model
test
set
comparison
k
fold
cross
validation
method
randomly
partitions
data
into
k
subsets
then
k
experiments
performed
each
respectively
considering


subset
evaluation
remaining
k


subsets
training
model
addition
holdout
cross
validation
methods
bootstrap
which
samples
n
instances
replacement
from
dataset
used
assess
model
accuracy


addition
overall
accuracy
investigators
frequently
report
sensitivity
specificity
meaning
true
positive
rate
tpr
true
negative
rate
tnr
respectively
similarly
investigators
sometimes
report
false
positive
rate
fpr
as
well
as
false
negative
rate
fnr
however
these
rates
ratios
fail
reveal
their
numerators
denominators
receiver
operating
characteristic
roc
along
accompanying
area
under
roc
curve
auc
offer
additional
tools
classification
model
assessment
higher
auc
associated
better
performing
model


ethics
artificial
intelligence
covers
broad
range
topics
within
ai
considered
particular
ethical
stakes


includes
algorithmic
biases
fairness


automated
decision
making


accountability
privacy
regulation
also
covers
various
emerging
potential
future
challenges
such
as
machine
ethics
how
make
machines
behave
ethically
lethal
autonomous
weapon
systems
arms
race
dynamics
ai
safety
alignment
technological
unemployment
ai
enabled
misinformation


how
treat
certain
ai
systems
if
moral
status
ai
welfare
rights
artificial
superintelligence
existential
risks


different
machine
learning
approaches
suffer
from
different
data
biases
machine
learning
system
trained
specifically
current
customers
not
able
predict
needs
new
customer
groups
not
represented
training
data
when
trained
human
made
data
machine
learning
likely
pick
up
constitutional
unconscious
biases
already
present
society


systems
trained
datasets
collected
biases
exhibit
these
biases
upon
use
algorithmic
bias
thus
digitising
cultural
prejudices


example


uk
s
commission
racial
equality
found
st
george
s
medical
school
using
computer
program
trained
from
data
previous
admissions
staff
program
denied
nearly


candidates
who
found
either
women
non
european
sounding
names


using
job
hiring
data
from
firm
racist
hiring
policies
lead
machine
learning
system
duplicating
bias
scoring
job
applicants
similarity
previous
successful
applicants




another
example
includes
predictive
policing
company
geolitica
s
predictive
algorithm
resulted
disproportionately
high
levels
over
policing
low
income
minority
communities
after
trained
historical
crime
data


while
responsible
collection
data
documentation
algorithmic
rules
used
system
considered
critical
part
machine
learning
some
researchers
blame
lack
participation
representation
minority
population
field
ai
machine
learning
s
vulnerability
biases


fact
according
research
carried
out
computing
research
association
cra


female
faculty
merely
make
up




all
faculty
members
who
focus
ai
among
several
universities
around
world


furthermore
among
group
new
u
s
resident
ai
phd
graduates


identified
as
white




as
asian




as
hispanic




as
african
american
which
further
demonstrates
lack
diversity
field
ai


language
models
learned
from
data
shown
contain
human
like
biases




because
human
languages
contain
biases
machines
trained
language
corpora
necessarily
also
learn
these
biases






microsoft
tested
tay
chatbot
learned
from
twitter
quickly
picked
up
racist
sexist
language


experiment
carried
out
propublica
investigative
journalism
organisation
machine
learning
algorithm
s
insight
into
recidivism
rates
among
prisoners
falsely
flagged
black
defendants
high
risk
twice
as
often
as
white
defendants




google
photos
once
tagged
couple
black
people
as
gorillas
which
caused
controversy
gorilla
label
subsequently
removed


still
cannot
recognise
gorillas


similar
issues
recognising
non
white
people
found
many
other
systems


because
such
challenges
effective
use
machine
learning
take
longer
adopted
other
domains


concern
fairness
machine
learning
reducing
bias
machine
learning
propelling
its
use
human
good
increasingly
expressed
artificial
intelligence
scientists
including
fei
fei
li
who
said
t
here
s
nothing
artificial
ai
s
inspired
people
s
created
people
most
importantly
impacts
people
powerful
tool
only
just
beginning
understand
profound
responsibility


there
concerns
among
health
care
professionals
these
systems
not
designed
public
s
interest
as
income
generating
machines
especially
true
united
states
where
there
long
standing
ethical
dilemma
improving
health
care
also
increasing
profits
example
algorithms
designed
provide
patients
unnecessary
tests
medication
which
algorithm
s
proprietary
owners
hold
stakes
there
potential
machine
learning
health
care
provide
professionals
additional
tool
diagnose
medicate
plan
recovery
paths
patients
requires
these
biases
mitigated


since

s
advances
both
machine
learning
algorithms
computer
hardware
led
more
efficient
methods
training
deep
neural
networks
particular
narrow
subdomain
machine
learning
contain
many
layers
nonlinear
hidden
units




graphics
processing
units
gpus
often
ai
specific
enhancements
displaced
cpus
as
dominant
method
training
large
scale
commercial
cloud
ai


openai
estimated
hardware
compute
used
largest
deep
learning
projects
from
alexnet


alphazero


found




fold
increase
amount
compute
required
doubling
time
trendline




months




tensor
processing
units
tpus
specialised
hardware
accelerators
developed
google
specifically
machine
learning
workloads
unlike
general
purpose
gpus
fpgas
tpus
optimised
tensor
computations
making
particularly
efficient
deep
learning
tasks
such
as
training
inference
widely
used
google
cloud
ai
services
large
scale
machine
learning
models
like
google
s
deepmind
alphafold
large
language
models
tpus
leverage
matrix
multiplication
units
high
bandwidth
memory
accelerate
computations
while
maintaining
energy
efficiency


since
their
introduction


tpus
become
key
component
ai
infrastructure
especially
cloud
based
environments
neuromorphic
computing
refers
class
computing
systems
designed
emulate
structure
functionality
biological
neural
networks
these
systems
implemented
through
software
based
simulations
conventional
hardware
through
specialised
hardware
architectures


physical
neural
network
specific
type
neuromorphic
hardware
relies
electrically
adjustable
materials
such
as
memristors
emulate
function
neural
synapses
term
physical
neural
network
highlights
use
physical
hardware
computation
as
opposed
software
based
implementations
broadly
refers
artificial
neural
networks
use
materials
adjustable
resistance
replicate
neural
synapses




embedded
machine
learning
sub
field
machine
learning
where
models
deployed
embedded
systems
limited
computing
resources
such
as
wearable
computers
edge
devices
microcontrollers








running
models
directly
these
devices
eliminates
need
transfer
store
data
cloud
servers
further
processing
thereby
reducing
risk
data
breaches
privacy
leaks
theft
intellectual
property
personal
data
business
secrets
embedded
machine
learning
achieved
through
various
techniques
such
as
hardware
acceleration




approximate
computing


model
optimisation




common
optimisation
techniques
include
pruning
quantization
knowledge
distillation
low
rank
factorisation
network
architecture
search
parameter
sharing
software
sui